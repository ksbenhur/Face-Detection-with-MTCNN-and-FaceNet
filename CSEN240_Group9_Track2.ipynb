{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjOvZG_gFMQn"
      },
      "source": [
        "#### Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmn9gCJMme4R",
        "outputId": "e5fdcff3-ebdc-4a58-91dc-0e25030798f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2_3Ef5pFMQo"
      },
      "source": [
        "#### Setting up base paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "2XdI7eOZmqRv"
      },
      "outputs": [],
      "source": [
        "BASE_INPUT_PATH = \"/content/drive/MyDrive/SP24-CSEN240-2/Project\"\n",
        "TRAIN_DATA_PATH = BASE_INPUT_PATH + \"/Train\"\n",
        "VALIDATION_DATA_PATH = BASE_INPUT_PATH + \"/Validation\"\n",
        "TEST_DATA_PATH = BASE_INPUT_PATH + \"/Test\"\n",
        "GROUP_NUMBER = \"9\" # TODO\n",
        "ADDITIONAL_FILES_PATH = BASE_INPUT_PATH + \"/Temp/Group-%s\" % GROUP_NUMBER\n",
        "OUTPUT_PATH = BASE_INPUT_PATH +\"/Prediction\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import dependencies"
      ],
      "metadata": {
        "id": "HGczuObjRCRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mtcnn\n",
        "!pip install keras-facenet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzZeltBaJeTV",
        "outputId": "2622de79-afaf-4e29-f3cf-2541e15f3ddb"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mtcnn in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (2.15.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.25.2)\n",
            "Requirement already satisfied: keras-facenet in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: mtcnn in /usr/local/lib/python3.10/dist-packages (from keras-facenet) (0.1.1)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn->keras-facenet) (2.15.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn->keras-facenet) (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python>=4.1.0->mtcnn->keras-facenet) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "from keras_facenet import FaceNet\n",
        "embedder = FaceNet()\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import pickle\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ],
      "metadata": {
        "id": "0ya627TtRB-I"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing"
      ],
      "metadata": {
        "id": "fNkewk5FSoZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# TODO\n",
        "detector=MTCNN()\n",
        "def load_images_and_labels_with_face_detection(folder, label_mapping):\n",
        "    images = []\n",
        "    labels = []\n",
        "    print_counter = 5\n",
        "    for filename in sorted(os.listdir(folder)):\n",
        "        file_number = filename.split('.')[0]\n",
        "        img_path = os.path.join(folder, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is not None:\n",
        "            results = detector.detect_faces(img)\n",
        "            if results:  # Check if any faces were detected\n",
        "                x, y, w, h = results[0]['box']  # Get coordinates of the first detected face\n",
        "                x, y = abs(x), abs(y)  # Ensure coordinates are positive\n",
        "                face = img[y:y + h, x:x + w]  # Extract face region\n",
        "                face = cv2.resize(face, (128, 128))  # Resize to consistent size\n",
        "                images.append(face)\n",
        "                # Determine label based on filename or folder structure (adjust as needed)\n",
        "                label = label_mapping[file_number]\n",
        "                labels.append(label)\n",
        "                cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "                cv2.waitKey(0)\n",
        "            else:\n",
        "                face=img\n",
        "                face = cv2.resize(face, (128, 128))\n",
        "                images.append(face)\n",
        "                label = label_mapping[file_number]\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "\n",
        "\n",
        "# Preprocessig the test data\n",
        "# read images and labels, and detect the face boundary\n"
      ],
      "metadata": {
        "id": "qXMUAGwcSn9V"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FACELOADING:\n",
        "    def __init__(self, directory):\n",
        "        self.directory = directory\n",
        "        self.target_size = (160,160)\n",
        "        self.X = []\n",
        "        self.Y = []\n",
        "        self.detector = MTCNN()\n",
        "\n",
        "    def AB(self):\n",
        "        self.X.extend(test_images)\n",
        "        self.Y.extend(test_labels_encoded)\n",
        "        return np.asarray(self.X), np.asarray(self.Y)\n",
        "\n",
        "    def plot_images(self):\n",
        "        plt.figure(figsize=(18,16))\n",
        "        for num,image in enumerate(self.X):\n",
        "            ncols = 3\n",
        "            nrows = len(self.Y)//ncols + 1\n",
        "            plt.subplot(nrows,ncols,num+1)\n",
        "            plt.imshow(image)\n",
        "            plt.axis('off')"
      ],
      "metadata": {
        "id": "oXa1sQvAQw7W"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Pickle Files"
      ],
      "metadata": {
        "id": "SvGrYTivvY32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def load_from_pickle(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "train_images = load_from_pickle(ADDITIONAL_FILES_PATH +  '/train_images.pkl')\n",
        "train_labels = load_from_pickle(ADDITIONAL_FILES_PATH +  '/train_labels.pkl')\n",
        "\n",
        "\n",
        "filename_xy = os.path.join(ADDITIONAL_FILES_PATH, 'embedded_xy.pkl')\n",
        "filename_uv = os.path.join(ADDITIONAL_FILES_PATH, 'embedded_uv.pkl')\n",
        "with open(filename_xy, 'rb') as f:\n",
        "    loaded_data = pickle.load(f)\n",
        "    EMBEDDED_X, Y = loaded_data\n",
        "with open(filename_uv, 'rb') as f:\n",
        "    loaded_data = pickle.load(f)\n",
        "    EMBEDDED_U, V = loaded_data\n",
        "\n"
      ],
      "metadata": {
        "id": "6bYm9JICLSl8"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "train_labels = le.fit_transform(train_labels)\n",
        "#val_labels = le.transform(val_labels)\n",
        "#train_labels=sorted(train_labels)\n"
      ],
      "metadata": {
        "id": "9DWqM-S9VXLD"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading Test Images"
      ],
      "metadata": {
        "id": "mchjBe1YvJlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels_df = pd.read_csv(os.path.join(TEST_DATA_PATH, \"labels.txt\"), delimiter=\" \", header=None, dtype = {0: str})\n",
        "test_labels_df.columns = [\"file_number\", \"person_name\"]\n",
        "# Create a dictionary to map file numbers to person names\n",
        "test_label_mapping = test_labels_df.set_index('file_number')['person_name'].to_dict()\n",
        "\n",
        "# Preprocessig the test data\n",
        "# read images and labels, and detect the face boundary\n",
        "test_images, test_labels = load_images_and_labels_with_face_detection(TEST_DATA_PATH, test_label_mapping)\n",
        "# label encoding for test data\n",
        "test_labels_encoded = le.transform(test_labels)\n",
        "#test_labels_encoded=sorted(test_labels_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI3SjhjVhInJ",
        "outputId": "07c99d7a-23f0-48d9-e820-9de1f69ced18"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 224ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "2/2 [==============================] - 0s 10ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 213 calls to <function Model.make_predict_function.<locals>.predict_function at 0x79e6ac426dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 372ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "3/3 [==============================] - 0s 5ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 194ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(face_img):\n",
        "    face_img = face_img.astype('float32') # 3D(160x160x3)\n",
        "    face_img = np.expand_dims(face_img, axis=0)\n",
        "    # 4D (Nonex160x160x3)\n",
        "    yhat= embedder.embeddings(face_img)\n",
        "    return yhat[0] # 512D image (1x1x512)\n",
        "\n",
        "faceevolving=FACELOADING(TEST_DATA_PATH)\n",
        "A,B= faceevolving.AB()"
      ],
      "metadata": {
        "id": "rocnyGNjR3tX"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "Y = encoder.fit_transform(Y)\n",
        "V= encoder.transform(V)\n",
        "B= encoder.transform(B)"
      ],
      "metadata": {
        "id": "k3X0QhYnLfDs"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDED_A = []\n",
        "\n",
        "for img in A:\n",
        "    EMBEDDED_A.append(get_embedding(img))\n",
        "\n",
        "EMBEDDED_A = np.asarray(EMBEDDED_A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2AwtA5xLRF0",
        "outputId": "65baf2bd-f517-4df1-e8ec-49a0794a4c1a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate model performance reports"
      ],
      "metadata": {
        "id": "-GwAgEtTR4Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = SVC(kernel='rbf', probability=True)\n",
        "model.fit(EMBEDDED_X, Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "Nm-OsT_8VUD0",
        "outputId": "c6986db7-789e-4d15-ac0b-b09d228e0c9a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(probability=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ypreds_train = model.predict(EMBEDDED_X)\n",
        "ypreds_val = model.predict(EMBEDDED_U)\n",
        "ypreds_test = model.predict(EMBEDDED_A)"
      ],
      "metadata": {
        "id": "zAfb-WjBQw0r"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"Training Accuracy:\",accuracy_score(Y, ypreds_train))\n",
        "print(\"Validation Accuracy:\",accuracy_score(V,ypreds_val))\n",
        "print(\"Test Accuracy:\",accuracy_score(B,ypreds_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnKtFGvTWuOI",
        "outputId": "5b06029c-f994-4b55-d5fd-3b94ef8a1021"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 1.0\n",
            "Validation Accuracy: 0.9940119760479041\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C8yfoHwW8x43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate prediction file"
      ],
      "metadata": {
        "id": "3CsU5LHrRjk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "Vprednames=le.inverse_transform(ypreds_test)\n",
        "test_labels_df = pd.read_csv(os.path.join(TEST_DATA_PATH, \"labels.txt\"), delimiter=\" \", header=None, dtype={0: str})\n",
        "test_labels_df.columns = [\"file_number\", \"person_name\"]\n",
        "\n",
        "predicted_mapping = {file_num: pred for file_num, pred in zip(test_labels_df['file_number'], Vprednames)}\n",
        "    # print(predicted_mapping)\n",
        "df = pd.DataFrame(list(predicted_mapping.items()), columns=['file_number', 'predicted'])\n",
        "    # print(\"df\", df)\n",
        "df.to_csv(OUTPUT_PATH + '/' + 'Group-9-track-2.txt', sep=' ', header=False, index=False)\n"
      ],
      "metadata": {
        "id": "VGt7Kq2sly6x"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "with open(TEST_DATA_PATH+'/labels.txt', 'r') as f:\n",
        "    y_test = [line.strip().split(' ')[1] for line in f.readlines()]\n",
        "\n",
        "with open(OUTPUT_PATH+'/Group-9-track-2.txt', 'r') as f:\n",
        "    y_pred = [line.strip().split(' ')[1] for line in f.readlines()]\n",
        "\n",
        "score = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy score: %s'% score)"
      ],
      "metadata": {
        "id": "OMZkAsfI9ulP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ab8989-6fed-47af-a8d7-62f6fd8a71bb"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score: 1.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}